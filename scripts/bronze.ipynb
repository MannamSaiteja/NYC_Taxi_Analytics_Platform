{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "833b92a0-ca44-4c1e-bdf5-cecb1633a8cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6322fd87-78ba-49d8-8421-425ec7eab04d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configs\n",
    "catalog='nyc_taxi'\n",
    "schema='nyc_schema'\n",
    "raw_folder=f'/Volumes/{catalog}/{schema}/raw_zone/'\n",
    "bronze_path=f'/Volumes/{catalog}/{schema}/bronze_zone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cab229f-5016-45a9-b666-8f257f4bdbd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# expected schema\n",
    "expected_schema = StructType([\n",
    "    StructField(\"VendorID\", IntegerType(), True),\n",
    "    StructField(\"lpep_pickup_datetime\", TimestampType(), True),\n",
    "    StructField(\"lpep_dropoff_datetime\", TimestampType(), True),\n",
    "    StructField(\"store_and_fwd_flag\", StringType(), True),\n",
    "    StructField(\"RatecodeID\", IntegerType(), True),\n",
    "    StructField(\"PULocationID\", IntegerType(), True),\n",
    "    StructField(\"DOLocationID\", IntegerType(), True),\n",
    "    StructField(\"passenger_count\", IntegerType(), True),\n",
    "    StructField(\"trip_distance\", DoubleType(), True),\n",
    "    StructField(\"fare_amount\", DoubleType(), True),\n",
    "    StructField(\"extra\", DoubleType(), True),\n",
    "    StructField(\"mta_tax\", DoubleType(), True),\n",
    "    StructField(\"tip_amount\", DoubleType(), True),\n",
    "    StructField(\"tolls_amount\", DoubleType(), True),\n",
    "    StructField(\"ehail_fee\", DoubleType(), True),\n",
    "    StructField(\"improvement_surcharge\", DoubleType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"payment_type\", IntegerType(), True),\n",
    "    StructField(\"trip_type\", IntegerType(), True),\n",
    "    StructField(\"congestion_surcharge\", DoubleType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a9c8234-d477-4404-bb42-07a42ab0d9eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#raw files list\n",
    "files = [f for f in dbutils.fs.ls(raw_folder) if f.name.endswith(\".parquet\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c65eac-7d12-4130-a706-db758a46afd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    file_path = file.path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_date = file_name.replace(\"green_tripdata_\", \"\").replace(\".parquet\", \"\")\n",
    "    print(f\"\\n--- Processing file: {file_name} ---\")\n",
    "    df_raw = spark.read.parquet(file_path)\n",
    "\n",
    "    #Schema Validation\n",
    "    actual_cols = df_raw.columns\n",
    "    expected_cols = [f.name for f in expected_schema]\n",
    "    missing_cols = set(expected_cols) - set(actual_cols)\n",
    "    unexpected_cols = set(actual_cols) - set(expected_cols)\n",
    "\n",
    "    print(\"Missing Columns:\", missing_cols)\n",
    "    print(\"Unexpected Columns:\", unexpected_cols)\n",
    "\n",
    "    # Essential Columns Check\n",
    "    essential_columns = [\n",
    "        'VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime',\n",
    "        'RatecodeID', 'PULocationID', 'DOLocationID',\n",
    "        'passenger_count', 'trip_distance', 'fare_amount', 'payment_type'\n",
    "    ]\n",
    "    missing_essentials = [col for col in essential_columns if col not in df_raw.columns]\n",
    "    if missing_essentials:\n",
    "        print(\"Missing essential columns:\", missing_essentials)\n",
    "        continue\n",
    "    else:\n",
    "        print(\"All essential columns present.\")\n",
    "\n",
    "    # Null Count Logging (Optional in production)\n",
    "    null_counts = df_raw.select([\n",
    "        sum(when(col(c).isNull(), 1).otherwise(0)).alias(f\"{c}_null_count\")\n",
    "        for c in df_raw.columns\n",
    "    ])\n",
    "    print(f\"\\n Null Count Summary for {file_name}:\")\n",
    "    null_counts.show(truncate=False)\n",
    "\n",
    "    # File Traceability Columns\n",
    "    df_bronze = df_raw.withColumn(\"file_date\", lit(file_date)) \\\n",
    "                      .withColumn(\"source_file\", col(\"_metadata.file_path\")) \\\n",
    "                      .withColumn(\"ingestion_date\", current_date())\n",
    "\n",
    "    # Duplicate Logging\n",
    "    total_count = df_bronze.count()\n",
    "    distinct_count = df_bronze.dropDuplicates().count()\n",
    "    print(f\"Total Rows: {total_count}, Distinct Rows: {distinct_count}, Duplicate Rows: {total_count - distinct_count}\")\n",
    "\n",
    "    # Write to bronze zone (partitioned by file_date)\n",
    "    df_bronze.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"file_date\") \\\n",
    "        .saveAsTable(\"nyc_taxi.nyc_schema.bronze\")\n",
    "\n",
    "    print(f\"Successfully written {file_name} to Bronze layer.\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6891979712922512,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
